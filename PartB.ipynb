{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Project2-PartB.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "RgKlxpR-BeDx",
        "pycharm": {}
      },
      "source": [
        "#### Adamantios Zaras AM: 06\n",
        "#### Panagiotis Souranis AM: 17"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "UvMOu3Fd4z9P",
        "pycharm": {}
      },
      "source": [
        "# Description\n",
        "\n",
        "In this part of the project, "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "dJmPyewf44XC",
        "pycharm": {}
      },
      "source": [
        "# Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "CjV8FVDP5nAS",
        "pycharm": {}
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bu-On9DmdNy0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git clone https://github.com/hsoleimani/MLTM.git && \\\n",
        "pip install pyclustering"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ubj0XegWl_QS",
        "pycharm": {},
        "colab": {}
      },
      "source": [
        "import warnings\n",
        "from random import randint, sample\n",
        "\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import scipy\n",
        "import scipy.stats as sp\n",
        "from itertools import combinations\n",
        "from scipy.spatial.distance import directed_hausdorff as hausdorff_distance\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.metrics import accuracy_score, make_scorer\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import LinearSVC, SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from pyclustering.cluster.kmedoids import kmedoids\n",
        "\n",
        "from utils import hyperparameters_search\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ufwbjh1x58Pr",
        "pycharm": {}
      },
      "source": [
        "## Prepare Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8jVamWcBg-89",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_bags_of_sentences(documents_path: str, labels_path: str) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Creates a bag of sentences.\n",
        "\n",
        "    :param documents_path: the path to the documents.\n",
        "    :param labels_path: the path to the labels.\n",
        "    :return: Pandas Dataframe containing the bag of sentences.\n",
        "    \"\"\"\n",
        "    # Get the most frequent class only.\n",
        "    labels = pd.read_csv(labels_path, header=None)\n",
        "    labels = labels[0].map(lambda x: np.array([int(lab) for lab in x.split()]))\n",
        "    labels = np.array(labels.tolist())\n",
        "    most_frequent_counts = np.sum(np.transpose(labels), axis=1)\n",
        "    most_frequent_index = most_frequent_counts.argmax()\n",
        "    labels = labels[:, most_frequent_index]\n",
        "\n",
        "    # Open documents file.\n",
        "    documents_file = open(documents_path, 'r')\n",
        "\n",
        "    # Initialize counters and bag of sentences dictionary.\n",
        "    document_counter = 0\n",
        "    sentence_count = 0\n",
        "    bag_of_sentences = {}\n",
        "\n",
        "    for document, label in zip(documents_file, labels):\n",
        "        # Parse document.\n",
        "        parsed_document = re.split(r'<\\d+>', document)\n",
        "\n",
        "        for sentence in parsed_document:\n",
        "            # Remove leading and trailing whitespaces.\n",
        "            sentence = sentence.strip()\n",
        "\n",
        "            # If sentence is not empty.\n",
        "            if sentence:\n",
        "                # Store words to an array of ints, since they are just ids.\n",
        "                words = np.asarray(sentence.split(\" \"), dtype=np.int32)\n",
        "                # Add a sentence to the bag.\n",
        "                bag_of_sentences[sentence_count] = (document_counter, words, label)\n",
        "                sentence_count += 1\n",
        "\n",
        "        document_counter += 1\n",
        "\n",
        "    # Close documents file.\n",
        "    documents_file.close()\n",
        "\n",
        "    # Create dataframe of the bag.\n",
        "    df = pd.DataFrame.from_dict(bag_of_sentences, orient='index',\n",
        "                                columns = ['Bag', 'Sentence', 'Class'])\n",
        "\n",
        "    return df\n",
        "\n",
        "# Create bags of sentences for the train and test data.\n",
        "train_df = create_bags_of_sentences('MLTM/Data/Delicious/train-data.dat', \n",
        "                           'MLTM/Data/Delicious/train-label.dat')\n",
        "test_df = create_bags_of_sentences('MLTM/Data/Delicious/test-data.dat', \n",
        "                          'MLTM/Data/Delicious/test-label.dat')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4SxYKhvOvN14",
        "colab_type": "text"
      },
      "source": [
        "Demonstrate data structure."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJDDIxQxKtZq",
        "colab_type": "code",
        "outputId": "b7081dd1-29c6-4ba7-bbae-58eb2a5ee4d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        }
      },
      "source": [
        "train_df.head()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Bag</th>\n",
              "      <th>Sentence</th>\n",
              "      <th>Class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>[6705, 5997, 8310, 3606, 674, 8058, 5044, 4836]</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>[4312, 5154, 8310, 4225]</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>[1827, 1037, 8482, 483]</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>[3567, 6172, 6172, 2892, 1362, 787, 399, 777, ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>[318, 769, 4621, 3199, 1480, 6213, 971, 6890]</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Bag                                           Sentence  Class\n",
              "0    0    [6705, 5997, 8310, 3606, 674, 8058, 5044, 4836]      1\n",
              "1    0                           [4312, 5154, 8310, 4225]      1\n",
              "2    1                            [1827, 1037, 8482, 483]      1\n",
              "3    1  [3567, 6172, 6172, 2892, 1362, 787, 399, 777, ...      1\n",
              "4    1      [318, 769, 4621, 3199, 1480, 6213, 971, 6890]      1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KZOytc7XvNJ9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_bag_per_document(df: pd.DataFrame):\n",
        "    \"\"\"\n",
        "    Parse a sentences dataframe \n",
        "    and get a bag of sentences for each document, with its labels.\n",
        "\n",
        "    :param df: the dataframe.\n",
        "    :return: the data and the labels.\n",
        "    \"\"\"\n",
        "    ids, X, y = np.array(df['Bag']), np.array(df['Sentence']), np.array(df['Class'])\n",
        "    un_id = np.unique(ids)\n",
        "    data = []\n",
        "    labels = []\n",
        "\n",
        "    for i in range(un_id.shape[0]):\n",
        "        bag = X[np.where(ids == i)]\n",
        "        data.append(bag)\n",
        "        label = y[np.where(ids == i)]\n",
        "        labels.append(label)\n",
        "\n",
        "    data = np.array(data)\n",
        "    labels = np.array(labels)\n",
        "    labels = np.array([labels[i][0] for i in range(labels.shape[0])])\n",
        "\n",
        "    # Pad sentences for 40 words per sentence max.\n",
        "    for i, sentence in enumerate(data):\n",
        "      data[i] = pad_sequences(sentence, maxlen=40)\n",
        "\n",
        "    return data, labels\n",
        "\n",
        "# Get the bags and the labels.\n",
        "train_bag, y_train = create_bag_per_document(train_df)\n",
        "test_bag, y_test = create_bag_per_document(test_df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cOPQzXN516mo",
        "colab_type": "text"
      },
      "source": [
        "## Transform Problem"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3LmZtpbg2JY_",
        "colab_type": "text"
      },
      "source": [
        "In this section, we transform the problem, using the K-medoids approach."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2odb-BYmdSEG",
        "colab_type": "text"
      },
      "source": [
        "#### Distance matrix."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ADXKHhqYfhT2",
        "colab_type": "text"
      },
      "source": [
        "First we calculate the distance matrix."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fsenF-ce1-gg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def hausdorff_symmetric(x, y):\n",
        "    \"\"\"Calculate symmetric hausdorff distance.\"\"\"\n",
        "    return max(hausdorff_distance(x, y)[0], hausdorff_distance(y, x)[0])\n",
        "\n",
        "# Initialize distance matrix.\n",
        "n_data = train_bag.shape[0]\n",
        "distance_matrix = np.zeros((n_data, n_data))\n",
        "\n",
        "# Calculate symmetric haussdorff distances.\n",
        "for (i, x), (j, y) in combinations(enumerate(train_bag), 2):\n",
        "    distance_matrix[i, j] = hausdorff_symmetric(x, y)\n",
        "    distance_matrix[j, i] = distance_matrix[i, j]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WAI5M7vnd2wx",
        "colab_type": "text"
      },
      "source": [
        "#### Cluster"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pnzIqW7ifY_2",
        "colab_type": "text"
      },
      "source": [
        "Then we cluster the bags in 15 clusters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b8BZkq0Od3RQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Randomly initialize medoids.\n",
        "initial_medoid_indices = sample(range(distance_matrix.shape[0]), 15)\n",
        "\n",
        "# Run K-Medoids and get the final medoids.\n",
        "k_medoids = kmedoids(distance_matrix, initial_medoid_indices, data_type='distance_matrix')\n",
        "k_medoids.process()\n",
        "final_medoids = train_bag[k_medoids.get_medoids()]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jcM3L935dobF",
        "colab_type": "text"
      },
      "source": [
        "#### Generate features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N87wXlB7fxI0",
        "colab_type": "text"
      },
      "source": [
        "Finally we transform the data, using the distances from the medoids as features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QTZXp_O-do2u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_features(data_bag, medoids) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Generates features from a bag of instances and some medoids.\n",
        "\n",
        "    :param data_bag: the bag of instances.\n",
        "    :param medoids: the medoids features.\n",
        "    :return: a numpy array containing the data with their generated features.\n",
        "    \"\"\"\n",
        "    data_transformed = np.empty((len(data_bag), len(medoids)))\n",
        "\n",
        "    # Generate features, using the distances from the medoids.\n",
        "    for i, x in enumerate(data_bag):\n",
        "        for j, medoid in enumerate(medoids):\n",
        "            data_transformed[i][j] = hausdorff_symmetric(x, medoid)\n",
        "\n",
        "    # Normalize distance features to [0, 1].\n",
        "    scaler = MinMaxScaler()\n",
        "    scaler.fit_transform(data_transformed)\n",
        "\n",
        "    return data_transformed\n",
        "\n",
        "# Transform data.\n",
        "X_train_transformed = generate_features(train_bag, final_medoids)\n",
        "X_test_transformed = generate_features(test_bag, final_medoids)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J1Lk7XHKgqIp",
        "colab_type": "text"
      },
      "source": [
        "Demonstrate new dataset shape."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RV8WP4FDgnGZ",
        "colab": {}
      },
      "source": [
        "print('New X train data shape: {}'.format(X_train_transformed.shape))\n",
        "print('New X test  data shape: {}'.format(X_test_transformed.shape))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nhkyoXcHrwZ2",
        "colab_type": "text"
      },
      "source": [
        "# Hyperparameters search"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rtu6QTP0sfcM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define classifiers.\n",
        "classifiers = {\n",
        "        'SVM': LinearSVC(random_state=0),\n",
        "        'Tree': DecisionTreeClassifier(random_state=0),\n",
        "        'Bayes': MultinomialNB()\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zFt5qwY3sQb_",
        "colab_type": "text"
      },
      "source": [
        "## Random Search"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bHwGpq6Arz3c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create param dists.\n",
        "svm_param_dist = {'C': 10 ** np.random.uniform(-3, 3, size=7000)}\n",
        "tree_param_dist = {'max_depth': scipy.stats.randint(1, 30),\n",
        "                   'max_features': scipy.stats.randint(1, X_train_transformed.shape[1]),\n",
        "                   'min_samples_split': scipy.stats.randint(2, X_train_transformed.shape[0] / 2),\n",
        "                   'criterion': ['gini', 'entropy']\n",
        "}\n",
        "# Add param dists to a list.\n",
        "params_list = [svm_param_dist, tree_param_dist]\n",
        "\n",
        "# Perform random search.\n",
        "for key, classifier ,params in zip(classifiers.keys(), classifiers.values(), params_list):\n",
        "    hyperparameters_search(classifier, params, X_train_transformed, y_train, 'Accuracy',\n",
        "                           {'Accuracy': make_scorer(accuracy_score)}, key, \n",
        "                           candidates=100, cv=5, random_search=True, verbose=5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Ld18AbB14bqy"
      },
      "source": [
        "## Grid Search"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "awxmc3yx4bq-",
        "colab": {}
      },
      "source": [
        "# Create parameter grids.\n",
        "svm_grid = {'C': np.arange(110, 130, .5)}\n",
        "tree_grid = {\n",
        "        'max_depth': range(5, 9),\n",
        "        'max_features': range(45, 55),\n",
        "        'min_samples_split': range(2645, 2648),\n",
        "        'criterion': ['gini']\n",
        "}\n",
        "bayes_grid = {'alpha': np.arange(0, 10, 0.2)}\n",
        "# Add param grids to a list.\n",
        "params_list = [svm_grid, tree_grid, bayes_grid]\n",
        "\n",
        "# Perform grid search.\n",
        "for key, classifier ,params in zip(classifiers.keys(), classifiers.values(), params_list):\n",
        "    hyperparameters_search(classifier, params, X_train_transformed, y_train, 'Accuracy',\n",
        "                           {'Accuracy': make_scorer(accuracy_score)}, key, \n",
        "                           cv=10, random_search=False, verbose=10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AToVtWTGtIlt",
        "colab_type": "text"
      },
      "source": [
        "## Final classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-RD8o9IjtOt-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create the best classifier found from the search.\n",
        "clf = DecisionTreeClassifier(criterion='gini', max_depth=7, max_features=54, \n",
        "                             min_samples_split=2645, random_state=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gu1fGS5fKo83",
        "colab_type": "text"
      },
      "source": [
        "# Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jeOvRUY3YFDT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "clf.fit(X_train_transformed, y_train)\n",
        "y_pred = clf.predict(X_test_transformed)\n",
        "\n",
        "print('Final Results')\n",
        "print('---------------------')\n",
        "print('Accuracy       {:.4f}'\n",
        "      .format(accuracy_score(y_test, y_pred)))\n",
        "print('Precision      {:.4f}'\n",
        "      .format(precision_score(y_test, y_pred, average=averaging)))\n",
        "print('Recall         {:.4f}'\n",
        "      .format(recall_score(y_test, y_pred, average=averaging)))\n",
        "print('F1             {:.4f}'\n",
        "      .format(f1_score(y_test, y_pred, average=averaging)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2QM4Ol55KrF7",
        "colab_type": "text"
      },
      "source": [
        "# Conclusion"
      ]
    }
  ]
}