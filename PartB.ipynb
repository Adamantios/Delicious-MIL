{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Project2-PartB.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "RgKlxpR-BeDx",
        "pycharm": {}
      },
      "source": [
        "#### Adamantios Zaras AM: 06\n",
        "#### Panagiotis Souranis AM: 17"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "UvMOu3Fd4z9P",
        "pycharm": {}
      },
      "source": [
        "# Description\n",
        "\n",
        "In this part of the project, "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "dJmPyewf44XC",
        "pycharm": {}
      },
      "source": [
        "# Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "CjV8FVDP5nAS",
        "pycharm": {}
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bu-On9DmdNy0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git clone https://github.com/hsoleimani/MLTM.git && \\\n",
        "pip install pyclustering"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ubj0XegWl_QS",
        "pycharm": {},
        "colab": {}
      },
      "source": [
        "import warnings\n",
        "from random import randint, sample\n",
        "\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import scipy\n",
        "import scipy.stats as sp\n",
        "from scipy.spatial.distance import directed_hausdorff as hausdorff_distance\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.metrics import accuracy_score, make_scorer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import LinearSVC, SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from pyclustering.cluster.kmedoids import kmedoids\n",
        "from pyclustering.utils.metric import distance_metric, type_metric\n",
        "\n",
        "from utils import hyperparameters_search\n",
        "from k_medoids import KMedoidsHaussdorff\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ufwbjh1x58Pr",
        "pycharm": {}
      },
      "source": [
        "## Prepare Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8jVamWcBg-89",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_bags_of_sentences(documents_path: str, labels_path: str) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Creates a bag of sentences.\n",
        "\n",
        "    :param documents_path: the path to the documents.\n",
        "    :param labels_path: the path to the labels.\n",
        "    :return: Pandas Dataframe containing the bag of sentences.\n",
        "    \"\"\"\n",
        "    # Get the most frequent class only.\n",
        "    labels = pd.read_csv(labels_path, header=None)\n",
        "    labels = labels[0].map(lambda x: np.array([int(lab) for lab in x.split()]))\n",
        "    labels = np.array(labels.tolist())\n",
        "    most_frequent_counts = np.sum(np.transpose(labels), axis=1)\n",
        "    most_frequent_index = most_frequent_counts.argmax()\n",
        "    labels = labels[:, most_frequent_index]\n",
        "\n",
        "    # Open documents file.\n",
        "    documents_file = open(documents_path, 'r')\n",
        "\n",
        "    # Initialize counters and bag of sentences dictionary.\n",
        "    document_counter = 0\n",
        "    sentence_count = 0\n",
        "    bag_of_sentences = {}\n",
        "\n",
        "    for document, label in zip(documents_file, labels):\n",
        "        # Parse document.\n",
        "        parsed_document = re.split(r'<\\d+>', document)\n",
        "\n",
        "        for sentence in parsed_document:\n",
        "            # Remove leading and trailing whitespaces.\n",
        "            sentence = sentence.strip()\n",
        "\n",
        "            # If sentence is not empty.\n",
        "            if sentence:\n",
        "                # Store words to an array of ints, since they are just ids.\n",
        "                words = np.asarray(sentence.split(\" \"), dtype=np.int32)\n",
        "                # Add a sentence to the bag.\n",
        "                bag_of_sentences[sentence_count] = (document_counter, words, label)\n",
        "                sentence_count += 1\n",
        "\n",
        "        document_counter += 1\n",
        "\n",
        "    # Close documents file.\n",
        "    documents_file.close()\n",
        "\n",
        "    # Create dataframe of the bag.\n",
        "    df = pd.DataFrame.from_dict(bag_of_sentences, orient='index',\n",
        "                                columns = ['Bag', 'Sentence', 'Class'])\n",
        "\n",
        "    return df\n",
        "\n",
        "# Create bags of sentences for the train and test data.\n",
        "train_df = create_bags_of_sentences('MLTM/Data/Delicious/train-data.dat', \n",
        "                           'MLTM/Data/Delicious/train-label.dat')\n",
        "test_df = create_bags_of_sentences('MLTM/Data/Delicious/test-data.dat', \n",
        "                          'MLTM/Data/Delicious/test-label.dat')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4SxYKhvOvN14",
        "colab_type": "text"
      },
      "source": [
        "Demonstrate data structure."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJDDIxQxKtZq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "21ec129c-b4a6-4ddd-ac8c-e981719a2e00"
      },
      "source": [
        "train_df.head()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Bag</th>\n",
              "      <th>Sentence</th>\n",
              "      <th>Class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>[6705, 5997, 8310, 3606, 674, 8058, 5044, 4836]</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>[4312, 5154, 8310, 4225]</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>[1827, 1037, 8482, 483]</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>[3567, 6172, 6172, 2892, 1362, 787, 399, 777, ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>[318, 769, 4621, 3199, 1480, 6213, 971, 6890]</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Bag                                           Sentence  Class\n",
              "0    0    [6705, 5997, 8310, 3606, 674, 8058, 5044, 4836]      1\n",
              "1    0                           [4312, 5154, 8310, 4225]      1\n",
              "2    1                            [1827, 1037, 8482, 483]      1\n",
              "3    1  [3567, 6172, 6172, 2892, 1362, 787, 399, 777, ...      1\n",
              "4    1      [318, 769, 4621, 3199, 1480, 6213, 971, 6890]      1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KZOytc7XvNJ9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_bag_per_document(df: pd.DataFrame):\n",
        "    \"\"\"\n",
        "    Parse a sentences dataframe \n",
        "    and get a bag of sentences for each document, with its labels.\n",
        "\n",
        "    :param df: the dataframe.\n",
        "    :return: the data and the labels.\n",
        "    \"\"\"\n",
        "    ids, X, y = np.array(df['Bag']), np.array(df['Sentence']), np.array(df['Class'])\n",
        "\n",
        "    X = pad_sequences(X, maxlen=200)\n",
        "    un_id = np.unique(ids)\n",
        "\n",
        "    data = []\n",
        "    labels = []\n",
        "    for i in range(un_id.shape[0]):\n",
        "        bag = X[np.where(ids == i)]\n",
        "        data.append(bag)\n",
        "        label = y[np.where(ids == i)]\n",
        "        labels.append(label)\n",
        "    data = np.array(data)\n",
        "    labels = np.array(labels)\n",
        "    labels = np.array([labels[i][0] for i in range(labels.shape[0])])\n",
        "\n",
        "    return data, labels\n",
        "\n",
        "# Get the bags and the labels.\n",
        "train_bag, y_train = create_bag_per_document(train_df)\n",
        "test_bag, y_test = create_bag_per_document(test_df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cOPQzXN516mo",
        "colab_type": "text"
      },
      "source": [
        "## Transform Problem"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3LmZtpbg2JY_",
        "colab_type": "text"
      },
      "source": [
        "We transform the problem, using the K-medoids approach."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fsenF-ce1-gg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "ec818904-5175-4c26-9652-f4b7986177c8"
      },
      "source": [
        "def hausdorff_symmetric(x, y):\n",
        "    \"\"\"Calculate symmetric hausdorff distance.\"\"\"\n",
        "    return max(hausdorff_distance(x, y)[0], hausdorff_distance(y, x)[0])\n",
        "\n",
        "# Initialize distances matrix.\n",
        "print('Calculating Hausdorff distances...')\n",
        "n_data = train_bag.shape[0]\n",
        "distance_matrix = np.empty((n_data, n_data))\n",
        "\n",
        "# Calculate symmetric haussdorff distances.\n",
        "for outer, x in enumerate(train_bag):\n",
        "    for inner, y in enumerate(train_bag):\n",
        "        distance_matrix[outer, inner] = hausdorff_symmetric(x, y)\n",
        "\n",
        "# Set number of clusters.\n",
        "k = 3\n",
        "print('Applying K Medoids for {} clusters...'.format(k))\n",
        "# Randomly initialize initial medoids.\n",
        "initial_medoid_indices = sample(range(distance_matrix.shape[0]), k)\n",
        "# Run K-Medoids and get the final medoids.\n",
        "k_medoids = kmedoids(distance_matrix, initial_medoid_indices, data_type='distance_matrix')\n",
        "k_medoids.process()\n",
        "final_medoids = train_bag[k_medoids.get_medoids()]\n",
        "\n",
        "def generate_features(data_bag, medoids) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Generates features from a bag of instances and some medoids.\n",
        "\n",
        "    :param data_bag: the bag of instances.\n",
        "    :param medoids: the medoids features.\n",
        "    :return: a numpy array containing the data with their generated features.\n",
        "    \"\"\"\n",
        "    data_transformed = np.empty((len(data_bag), len(medoids)))\n",
        "\n",
        "    # Generate features, using the distances from the medoids.\n",
        "    for i, x in enumerate(data_bag):\n",
        "        for j, medoid in enumerate(medoids):\n",
        "            data_transformed[i][j] = hausdorff_symmetric(x, medoid)\n",
        "\n",
        "    # Normalize distance features to [0, 1].\n",
        "    scaler = MinMaxScaler()\n",
        "    scaler.fit_transform(data_transformed)\n",
        "\n",
        "    return data_transformed\n",
        "\n",
        "# Transform data.\n",
        "print('Transforming data...')\n",
        "X_train_transformed = generate_features(train_bag, final_medoids)\n",
        "X_test_transformed = generate_features(test_bag, final_medoids)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Calculating Hausdorff distances...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J1Lk7XHKgqIp",
        "colab_type": "text"
      },
      "source": [
        "Demonstrate new dataset shape."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RV8WP4FDgnGZ",
        "colab": {}
      },
      "source": [
        "print('New X train data shape: {}'.format(X_train_transformed.shape))\n",
        "print('New X test  data shape: {}'.format(X_test_transformed.shape))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nhkyoXcHrwZ2",
        "colab_type": "text"
      },
      "source": [
        "# Hyperparameters search"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rtu6QTP0sfcM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define classifiers.\n",
        "classifiers = {\n",
        "        'SVM': LinearSVC(random_state=0),\n",
        "        'Tree': DecisionTreeClassifier(random_state=0),\n",
        "        'Bayes': MultinomialNB()\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zFt5qwY3sQb_",
        "colab_type": "text"
      },
      "source": [
        "## Random Search"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bHwGpq6Arz3c",
        "colab_type": "code",
        "outputId": "e4865c52-82e1-4e01-dcf8-db721718c90d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 531
        }
      },
      "source": [
        "# Create param dists.\n",
        "svm_param_dist = {'C': 10 ** np.random.uniform(-3, 3, size=7000)}\n",
        "tree_param_dist = {'max_depth': scipy.stats.randint(1, 30),\n",
        "                   'max_features': scipy.stats.randint(1, X_train_transformed.shape[1]),\n",
        "                   'min_samples_split': scipy.stats.randint(2, X_train_transformed.shape[0] / 2),\n",
        "                   'criterion': ['gini', 'entropy']\n",
        "}\n",
        "# Add param dists to a list.\n",
        "params_list = [svm_param_dist, tree_param_dist]\n",
        "\n",
        "# Perform random search.\n",
        "for key, classifier ,params in zip(classifiers.keys(), classifiers.values(), params_list):\n",
        "    hyperparameters_search(classifier, params, X_train_transformed, y_train, 'Accuracy',\n",
        "                           {'Accuracy': make_scorer(accuracy_score)}, key, \n",
        "                           candidates=100, cv=5, random_search=True, verbose=5)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Εstimator : SVM\n",
            "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  14 tasks      | elapsed:  1.1min\n",
            "[Parallel(n_jobs=-1)]: Done  68 tasks      | elapsed:  5.2min\n",
            "[Parallel(n_jobs=-1)]: Done 158 tasks      | elapsed: 11.8min\n",
            "[Parallel(n_jobs=-1)]: Done 284 tasks      | elapsed: 21.1min\n",
            "[Parallel(n_jobs=-1)]: Done 446 tasks      | elapsed: 33.1min\n",
            "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed: 37.0min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Best parameters found for Estimator : SVM\n",
            "{'C': 118.2089155014123}\n",
            "\n",
            "Best score found for Accuracy Score metric : 0.560\n",
            "\n",
            "Εstimator : Tree\n",
            "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  19 tasks      | elapsed:    5.8s\n",
            "[Parallel(n_jobs=-1)]: Done  73 tasks      | elapsed:   19.3s\n",
            "[Parallel(n_jobs=-1)]: Done 163 tasks      | elapsed:   37.3s\n",
            "[Parallel(n_jobs=-1)]: Done 289 tasks      | elapsed:  1.1min\n",
            "[Parallel(n_jobs=-1)]: Done 472 tasks      | elapsed:  1.8min\n",
            "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed:  2.0min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Best parameters found for Estimator : Tree\n",
            "{'criterion': 'entropy', 'max_depth': 7, 'max_features': 49, 'min_samples_split': 2646}\n",
            "\n",
            "Best score found for Accuracy Score metric : 0.616\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Ld18AbB14bqy"
      },
      "source": [
        "## Grid Search"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "awxmc3yx4bq-",
        "outputId": "3c51d273-a492-4e72-cbc3-3744e82fa96d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1872
        }
      },
      "source": [
        "# Create parameter grids.\n",
        "svm_grid = {'C': np.arange(110, 130, .5)}\n",
        "tree_grid = {\n",
        "        'max_depth': range(5, 9),\n",
        "        'max_features': range(45, 55),\n",
        "        'min_samples_split': range(2645, 2648),\n",
        "        'criterion': ['gini']\n",
        "}\n",
        "bayes_grid = {'alpha': np.arange(0, 10, 0.2)}\n",
        "# Add param grids to a list.\n",
        "params_list = [svm_grid, tree_grid, bayes_grid]\n",
        "\n",
        "# Perform grid search.\n",
        "for key, classifier ,params in zip(classifiers.keys(), classifiers.values(), params_list):\n",
        "    hyperparameters_search(classifier, params, X_train_transformed, y_train, 'Accuracy',\n",
        "                           {'Accuracy': make_scorer(accuracy_score)}, key, \n",
        "                           cv=10, random_search=False, verbose=10)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Εstimator : SVM\n",
            "Fitting 10 folds for each of 40 candidates, totalling 400 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:    9.6s\n",
            "[Parallel(n_jobs=-1)]: Done   4 tasks      | elapsed:   19.7s\n",
            "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:   49.4s\n",
            "[Parallel(n_jobs=-1)]: Done  14 tasks      | elapsed:  1.2min\n",
            "[Parallel(n_jobs=-1)]: Done  21 tasks      | elapsed:  1.8min\n",
            "[Parallel(n_jobs=-1)]: Done  28 tasks      | elapsed:  2.3min\n",
            "[Parallel(n_jobs=-1)]: Done  37 tasks      | elapsed:  3.2min\n",
            "[Parallel(n_jobs=-1)]: Done  46 tasks      | elapsed:  3.8min\n",
            "[Parallel(n_jobs=-1)]: Done  57 tasks      | elapsed:  4.8min\n",
            "[Parallel(n_jobs=-1)]: Done  68 tasks      | elapsed:  5.7min\n",
            "[Parallel(n_jobs=-1)]: Done  81 tasks      | elapsed:  6.9min\n",
            "[Parallel(n_jobs=-1)]: Done  94 tasks      | elapsed:  7.9min\n",
            "[Parallel(n_jobs=-1)]: Done 109 tasks      | elapsed:  9.2min\n",
            "[Parallel(n_jobs=-1)]: Done 124 tasks      | elapsed: 10.4min\n",
            "[Parallel(n_jobs=-1)]: Done 141 tasks      | elapsed: 11.9min\n",
            "[Parallel(n_jobs=-1)]: Done 158 tasks      | elapsed: 13.2min\n",
            "[Parallel(n_jobs=-1)]: Done 177 tasks      | elapsed: 14.9min\n",
            "[Parallel(n_jobs=-1)]: Done 196 tasks      | elapsed: 16.4min\n",
            "[Parallel(n_jobs=-1)]: Done 217 tasks      | elapsed: 18.2min\n",
            "[Parallel(n_jobs=-1)]: Done 238 tasks      | elapsed: 19.9min\n",
            "[Parallel(n_jobs=-1)]: Done 261 tasks      | elapsed: 21.9min\n",
            "[Parallel(n_jobs=-1)]: Done 284 tasks      | elapsed: 23.8min\n",
            "[Parallel(n_jobs=-1)]: Done 309 tasks      | elapsed: 25.9min\n",
            "[Parallel(n_jobs=-1)]: Done 334 tasks      | elapsed: 28.0min\n",
            "[Parallel(n_jobs=-1)]: Done 361 tasks      | elapsed: 30.3min\n",
            "[Parallel(n_jobs=-1)]: Done 388 tasks      | elapsed: 32.6min\n",
            "[Parallel(n_jobs=-1)]: Done 400 out of 400 | elapsed: 33.6min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Best parameters found for Estimator : SVM\n",
            "{'C': 110.0}\n",
            "\n",
            "Best score found for Accuracy Score metric : 0.505\n",
            "\n",
            "Εstimator : Tree\n",
            "Fitting 10 folds for each of 120 candidates, totalling 1200 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:    0.2s\n",
            "[Parallel(n_jobs=-1)]: Batch computation too fast (0.1638s.) Setting batch_size=2.\n",
            "[Parallel(n_jobs=-1)]: Done   4 tasks      | elapsed:    0.4s\n",
            "[Parallel(n_jobs=-1)]: Done  14 tasks      | elapsed:    1.3s\n",
            "[Parallel(n_jobs=-1)]: Done  24 tasks      | elapsed:    1.9s\n",
            "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:    3.1s\n",
            "[Parallel(n_jobs=-1)]: Done  52 tasks      | elapsed:    4.2s\n",
            "[Parallel(n_jobs=-1)]: Done  70 tasks      | elapsed:    5.8s\n",
            "[Parallel(n_jobs=-1)]: Done  88 tasks      | elapsed:    7.2s\n",
            "[Parallel(n_jobs=-1)]: Done 110 tasks      | elapsed:    9.2s\n",
            "[Parallel(n_jobs=-1)]: Done 132 tasks      | elapsed:   11.0s\n",
            "[Parallel(n_jobs=-1)]: Done 158 tasks      | elapsed:   13.4s\n",
            "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:   15.6s\n",
            "[Parallel(n_jobs=-1)]: Done 214 tasks      | elapsed:   18.2s\n",
            "[Parallel(n_jobs=-1)]: Done 244 tasks      | elapsed:   20.6s\n",
            "[Parallel(n_jobs=-1)]: Done 278 tasks      | elapsed:   23.2s\n",
            "[Parallel(n_jobs=-1)]: Done 312 tasks      | elapsed:   26.3s\n",
            "[Parallel(n_jobs=-1)]: Done 350 tasks      | elapsed:   29.5s\n",
            "[Parallel(n_jobs=-1)]: Done 388 tasks      | elapsed:   32.6s\n",
            "[Parallel(n_jobs=-1)]: Done 430 tasks      | elapsed:   36.7s\n",
            "[Parallel(n_jobs=-1)]: Done 472 tasks      | elapsed:   40.7s\n",
            "[Parallel(n_jobs=-1)]: Done 518 tasks      | elapsed:   44.9s\n",
            "[Parallel(n_jobs=-1)]: Done 564 tasks      | elapsed:   49.0s\n",
            "[Parallel(n_jobs=-1)]: Done 614 tasks      | elapsed:   53.4s\n",
            "[Parallel(n_jobs=-1)]: Done 664 tasks      | elapsed:   58.0s\n",
            "[Parallel(n_jobs=-1)]: Done 718 tasks      | elapsed:  1.1min\n",
            "[Parallel(n_jobs=-1)]: Done 772 tasks      | elapsed:  1.1min\n",
            "[Parallel(n_jobs=-1)]: Done 830 tasks      | elapsed:  1.2min\n",
            "[Parallel(n_jobs=-1)]: Done 888 tasks      | elapsed:  1.3min\n",
            "[Parallel(n_jobs=-1)]: Done 950 tasks      | elapsed:  1.4min\n",
            "[Parallel(n_jobs=-1)]: Done 1012 tasks      | elapsed:  1.5min\n",
            "[Parallel(n_jobs=-1)]: Done 1078 tasks      | elapsed:  1.6min\n",
            "[Parallel(n_jobs=-1)]: Done 1144 tasks      | elapsed:  1.7min\n",
            "[Parallel(n_jobs=-1)]: Done 1200 out of 1200 | elapsed:  1.8min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Best parameters found for Estimator : Tree\n",
            "{'criterion': 'gini', 'max_depth': 7, 'max_features': 54, 'min_samples_split': 2645}\n",
            "\n",
            "Best score found for Accuracy Score metric : 0.614\n",
            "\n",
            "Εstimator : Bayes\n",
            "Fitting 10 folds for each of 50 candidates, totalling 500 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:    0.1s\n",
            "[Parallel(n_jobs=-1)]: Batch computation too fast (0.0799s.) Setting batch_size=4.\n",
            "[Parallel(n_jobs=-1)]: Done   4 tasks      | elapsed:    0.2s\n",
            "[Parallel(n_jobs=-1)]: Done  24 tasks      | elapsed:    0.9s\n",
            "[Parallel(n_jobs=-1)]: Done  44 tasks      | elapsed:    1.4s\n",
            "[Parallel(n_jobs=-1)]: Done  72 tasks      | elapsed:    2.3s\n",
            "[Parallel(n_jobs=-1)]: Done 100 tasks      | elapsed:    3.2s\n",
            "[Parallel(n_jobs=-1)]: Done 136 tasks      | elapsed:    4.3s\n",
            "[Parallel(n_jobs=-1)]: Done 172 tasks      | elapsed:    5.4s\n",
            "[Parallel(n_jobs=-1)]: Done 216 tasks      | elapsed:    6.8s\n",
            "[Parallel(n_jobs=-1)]: Done 260 tasks      | elapsed:    8.1s\n",
            "[Parallel(n_jobs=-1)]: Done 312 tasks      | elapsed:    9.8s\n",
            "[Parallel(n_jobs=-1)]: Done 364 tasks      | elapsed:   11.3s\n",
            "[Parallel(n_jobs=-1)]: Done 424 tasks      | elapsed:   13.2s\n",
            "[Parallel(n_jobs=-1)]: Done 484 tasks      | elapsed:   15.1s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Best parameters found for Estimator : Bayes\n",
            "{'alpha': 4.800000000000001}\n",
            "\n",
            "Best score found for Accuracy Score metric : 0.491\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed:   15.5s finished\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AToVtWTGtIlt",
        "colab_type": "text"
      },
      "source": [
        "## Final classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-RD8o9IjtOt-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create the best classifier found from the search.\n",
        "clf = DecisionTreeClassifier(criterion='gini', max_depth=7, max_features=54, \n",
        "                             min_samples_split=2645, random_state=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gu1fGS5fKo83",
        "colab_type": "text"
      },
      "source": [
        "# Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jeOvRUY3YFDT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "clf.fit(X_train_transformed, y_train)\n",
        "y_pred = clf.predict(X_test_transformed)\n",
        "\n",
        "print('Final Results')\n",
        "print('---------------------')\n",
        "print('Accuracy       {:.4f}'\n",
        "      .format(accuracy_score(y_test, y_pred)))\n",
        "print('Precision      {:.4f}'\n",
        "      .format(precision_score(y_test, y_pred, average=averaging)))\n",
        "print('Recall         {:.4f}'\n",
        "      .format(recall_score(y_test, y_pred, average=averaging)))\n",
        "print('F1             {:.4f}'\n",
        "      .format(f1_score(y_test, y_pred, average=averaging)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2QM4Ol55KrF7",
        "colab_type": "text"
      },
      "source": [
        "# Conclusion"
      ]
    }
  ]
}