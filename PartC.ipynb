{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "Project2-PartC.ipynb",
   "version": "0.3.2",
   "provenance": [],
   "collapsed_sections": [
    "nhkyoXcHrwZ2",
    "zFt5qwY3sQb_",
    "Ld18AbB14bqy",
    "AToVtWTGtIlt"
   ],
   "toc_visible": true
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RgKlxpR-BeDx",
    "pycharm": {}
   },
   "source": [
    "#### Adamantios Zaras AM: 06\n",
    "#### Panagiotis Souranis AM: 17"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UvMOu3Fd4z9P",
    "pycharm": {}
   },
   "source": [
    "# Description\n",
    "\n",
    "In this part of the project, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dJmPyewf44XC",
    "pycharm": {}
   },
   "source": [
    "# Global"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CjV8FVDP5nAS",
    "pycharm": {}
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "bu-On9DmdNy0",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "!pip install scikit-multilearn && \\\n",
    "git clone https://github.com/hsoleimani/MLTM.git"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "Ubj0XegWl_QS",
    "pycharm": {},
    "colab": {}
   },
   "source": [
    "import warnings\n",
    "from random import randint\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy\n",
    "import scipy.stats as sp\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import accuracy_score, make_scorer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from utils import load_dataset, hyperparameters_search\n",
    "warnings.filterwarnings(\"ignore\")"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ufwbjh1x58Pr",
    "pycharm": {}
   },
   "source": [
    "## Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "ktwINyCB5_vQ",
    "pycharm": {},
    "outputId": "c8ba063d-8e95-4404-f09a-c8c98c5e558d",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 219
    }
   },
   "source": [
    "# Load dataset.\n",
    "X_train, y_train, X_test, y_test, word_index = load_dataset(ngram_range=1, \n",
    "                                                            path='MLTM/Data/Delicious', \n",
    "                                                            maxlen=200, binary=True)\n",
    "\n",
    "# Split test set to test and unlabeled.\n",
    "print('Splitting test data to test and unlabeled sets.')\n",
    "X_unlabeled, X_test, y_hidden, y_test = train_test_split(X_test, y_test, \n",
    "                                                         test_size=.5, random_state=0)\n",
    "print('{} test sequences.'.format(X_test.shape[0]))\n",
    "print('{} unlabeled sequences.'.format(X_unlabeled.shape[0]))"
   ],
   "execution_count": 3,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "8251 train sequences\n",
      "3983 test sequences\n",
      "X_train shape: (8251, 200)\n",
      "X_test shape: (3983, 200)\n",
      "\n",
      "Getting the most frequent class...\n",
      "The most frequent class was the word 'reference', with 3181 appearances.\n",
      "Splitting test data to test and unlabeled sets.\n",
      "1992 test sequences.\n",
      "1991 unlabeled sequences.\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nhkyoXcHrwZ2",
    "colab_type": "text"
   },
   "source": [
    "# Hyperparameters search"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "rtu6QTP0sfcM",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "# Define classifiers.\n",
    "classifiers = {\n",
    "        'SVM': LinearSVC(random_state=0),\n",
    "        'Tree': DecisionTreeClassifier(random_state=0),\n",
    "        'Bayes': MultinomialNB()\n",
    "}"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zFt5qwY3sQb_",
    "colab_type": "text"
   },
   "source": [
    "## Random Search"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "bHwGpq6Arz3c",
    "colab_type": "code",
    "outputId": "e4865c52-82e1-4e01-dcf8-db721718c90d",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 531
    }
   },
   "source": [
    "# Create param dists.\n",
    "svm_param_dist = {'C': 10 ** np.random.uniform(-3, 3, size=7000)}\n",
    "tree_param_dist = {'max_depth': scipy.stats.randint(1, 30),\n",
    "                   'max_features': scipy.stats.randint(1, X_train.shape[1]),\n",
    "                   'min_samples_split': scipy.stats.randint(2, X_train.shape[0] / 2),\n",
    "                   'criterion': ['gini', 'entropy']\n",
    "}\n",
    "# Add param dists to a list.\n",
    "params_list = [svm_param_dist, tree_param_dist]\n",
    "\n",
    "# Perform random search.\n",
    "for key, classifier ,params in zip(classifiers.keys(), classifiers.values(), params_list):\n",
    "    hyperparameters_search(classifier, params, X_train, y_train, 'Accuracy',\n",
    "                           {'Accuracy': make_scorer(accuracy_score)}, key, \n",
    "                           candidates=100, cv=5, random_search=True, verbose=5)"
   ],
   "execution_count": 14,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "\n",
      "Εstimator : SVM\n",
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  14 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done  68 tasks      | elapsed:  5.2min\n",
      "[Parallel(n_jobs=-1)]: Done 158 tasks      | elapsed: 11.8min\n",
      "[Parallel(n_jobs=-1)]: Done 284 tasks      | elapsed: 21.1min\n",
      "[Parallel(n_jobs=-1)]: Done 446 tasks      | elapsed: 33.1min\n",
      "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed: 37.0min finished\n"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "Best parameters found for Estimator : SVM\n",
      "{'C': 118.2089155014123}\n",
      "\n",
      "Best score found for Accuracy Score metric : 0.560\n",
      "\n",
      "Εstimator : Tree\n",
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  19 tasks      | elapsed:    5.8s\n",
      "[Parallel(n_jobs=-1)]: Done  73 tasks      | elapsed:   19.3s\n",
      "[Parallel(n_jobs=-1)]: Done 163 tasks      | elapsed:   37.3s\n",
      "[Parallel(n_jobs=-1)]: Done 289 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 472 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed:  2.0min finished\n"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "Best parameters found for Estimator : Tree\n",
      "{'criterion': 'entropy', 'max_depth': 7, 'max_features': 49, 'min_samples_split': 2646}\n",
      "\n",
      "Best score found for Accuracy Score metric : 0.616\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ld18AbB14bqy"
   },
   "source": [
    "## Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "awxmc3yx4bq-",
    "outputId": "3c51d273-a492-4e72-cbc3-3744e82fa96d",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1872
    }
   },
   "source": [
    "# Create parameter grids.\n",
    "svm_grid = {'C': np.arange(110, 130, .5)}\n",
    "tree_grid = {\n",
    "        'max_depth': range(5, 9),\n",
    "        'max_features': range(45, 55),\n",
    "        'min_samples_split': range(2645, 2648),\n",
    "        'criterion': ['gini']\n",
    "}\n",
    "bayes_grid = {'alpha': np.arange(0, 10, 0.2)}\n",
    "# Add param grids to a list.\n",
    "params_list = [svm_grid, tree_grid, bayes_grid]\n",
    "\n",
    "# Perform grid search.\n",
    "for key, classifier ,params in zip(classifiers.keys(), classifiers.values(), params_list):\n",
    "    hyperparameters_search(classifier, params, X_train, y_train, 'Accuracy',\n",
    "                           {'Accuracy': make_scorer(accuracy_score)}, key, \n",
    "                           cv=10, random_search=False, verbose=10)"
   ],
   "execution_count": 15,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "\n",
      "Εstimator : SVM\n",
      "Fitting 10 folds for each of 40 candidates, totalling 400 fits\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:    9.6s\n",
      "[Parallel(n_jobs=-1)]: Done   4 tasks      | elapsed:   19.7s\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:   49.4s\n",
      "[Parallel(n_jobs=-1)]: Done  14 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done  21 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done  28 tasks      | elapsed:  2.3min\n",
      "[Parallel(n_jobs=-1)]: Done  37 tasks      | elapsed:  3.2min\n",
      "[Parallel(n_jobs=-1)]: Done  46 tasks      | elapsed:  3.8min\n",
      "[Parallel(n_jobs=-1)]: Done  57 tasks      | elapsed:  4.8min\n",
      "[Parallel(n_jobs=-1)]: Done  68 tasks      | elapsed:  5.7min\n",
      "[Parallel(n_jobs=-1)]: Done  81 tasks      | elapsed:  6.9min\n",
      "[Parallel(n_jobs=-1)]: Done  94 tasks      | elapsed:  7.9min\n",
      "[Parallel(n_jobs=-1)]: Done 109 tasks      | elapsed:  9.2min\n",
      "[Parallel(n_jobs=-1)]: Done 124 tasks      | elapsed: 10.4min\n",
      "[Parallel(n_jobs=-1)]: Done 141 tasks      | elapsed: 11.9min\n",
      "[Parallel(n_jobs=-1)]: Done 158 tasks      | elapsed: 13.2min\n",
      "[Parallel(n_jobs=-1)]: Done 177 tasks      | elapsed: 14.9min\n",
      "[Parallel(n_jobs=-1)]: Done 196 tasks      | elapsed: 16.4min\n",
      "[Parallel(n_jobs=-1)]: Done 217 tasks      | elapsed: 18.2min\n",
      "[Parallel(n_jobs=-1)]: Done 238 tasks      | elapsed: 19.9min\n",
      "[Parallel(n_jobs=-1)]: Done 261 tasks      | elapsed: 21.9min\n",
      "[Parallel(n_jobs=-1)]: Done 284 tasks      | elapsed: 23.8min\n",
      "[Parallel(n_jobs=-1)]: Done 309 tasks      | elapsed: 25.9min\n",
      "[Parallel(n_jobs=-1)]: Done 334 tasks      | elapsed: 28.0min\n",
      "[Parallel(n_jobs=-1)]: Done 361 tasks      | elapsed: 30.3min\n",
      "[Parallel(n_jobs=-1)]: Done 388 tasks      | elapsed: 32.6min\n",
      "[Parallel(n_jobs=-1)]: Done 400 out of 400 | elapsed: 33.6min finished\n"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "Best parameters found for Estimator : SVM\n",
      "{'C': 110.0}\n",
      "\n",
      "Best score found for Accuracy Score metric : 0.505\n",
      "\n",
      "Εstimator : Tree\n",
      "Fitting 10 folds for each of 120 candidates, totalling 1200 fits\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.1638s.) Setting batch_size=2.\n",
      "[Parallel(n_jobs=-1)]: Done   4 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=-1)]: Done  14 tasks      | elapsed:    1.3s\n",
      "[Parallel(n_jobs=-1)]: Done  24 tasks      | elapsed:    1.9s\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:    3.1s\n",
      "[Parallel(n_jobs=-1)]: Done  52 tasks      | elapsed:    4.2s\n",
      "[Parallel(n_jobs=-1)]: Done  70 tasks      | elapsed:    5.8s\n",
      "[Parallel(n_jobs=-1)]: Done  88 tasks      | elapsed:    7.2s\n",
      "[Parallel(n_jobs=-1)]: Done 110 tasks      | elapsed:    9.2s\n",
      "[Parallel(n_jobs=-1)]: Done 132 tasks      | elapsed:   11.0s\n",
      "[Parallel(n_jobs=-1)]: Done 158 tasks      | elapsed:   13.4s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:   15.6s\n",
      "[Parallel(n_jobs=-1)]: Done 214 tasks      | elapsed:   18.2s\n",
      "[Parallel(n_jobs=-1)]: Done 244 tasks      | elapsed:   20.6s\n",
      "[Parallel(n_jobs=-1)]: Done 278 tasks      | elapsed:   23.2s\n",
      "[Parallel(n_jobs=-1)]: Done 312 tasks      | elapsed:   26.3s\n",
      "[Parallel(n_jobs=-1)]: Done 350 tasks      | elapsed:   29.5s\n",
      "[Parallel(n_jobs=-1)]: Done 388 tasks      | elapsed:   32.6s\n",
      "[Parallel(n_jobs=-1)]: Done 430 tasks      | elapsed:   36.7s\n",
      "[Parallel(n_jobs=-1)]: Done 472 tasks      | elapsed:   40.7s\n",
      "[Parallel(n_jobs=-1)]: Done 518 tasks      | elapsed:   44.9s\n",
      "[Parallel(n_jobs=-1)]: Done 564 tasks      | elapsed:   49.0s\n",
      "[Parallel(n_jobs=-1)]: Done 614 tasks      | elapsed:   53.4s\n",
      "[Parallel(n_jobs=-1)]: Done 664 tasks      | elapsed:   58.0s\n",
      "[Parallel(n_jobs=-1)]: Done 718 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 772 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 830 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 888 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 950 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 1012 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 1078 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done 1144 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done 1200 out of 1200 | elapsed:  1.8min finished\n"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "Best parameters found for Estimator : Tree\n",
      "{'criterion': 'gini', 'max_depth': 7, 'max_features': 54, 'min_samples_split': 2645}\n",
      "\n",
      "Best score found for Accuracy Score metric : 0.614\n",
      "\n",
      "Εstimator : Bayes\n",
      "Fitting 10 folds for each of 50 candidates, totalling 500 fits\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.0799s.) Setting batch_size=4.\n",
      "[Parallel(n_jobs=-1)]: Done   4 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done  24 tasks      | elapsed:    0.9s\n",
      "[Parallel(n_jobs=-1)]: Done  44 tasks      | elapsed:    1.4s\n",
      "[Parallel(n_jobs=-1)]: Done  72 tasks      | elapsed:    2.3s\n",
      "[Parallel(n_jobs=-1)]: Done 100 tasks      | elapsed:    3.2s\n",
      "[Parallel(n_jobs=-1)]: Done 136 tasks      | elapsed:    4.3s\n",
      "[Parallel(n_jobs=-1)]: Done 172 tasks      | elapsed:    5.4s\n",
      "[Parallel(n_jobs=-1)]: Done 216 tasks      | elapsed:    6.8s\n",
      "[Parallel(n_jobs=-1)]: Done 260 tasks      | elapsed:    8.1s\n",
      "[Parallel(n_jobs=-1)]: Done 312 tasks      | elapsed:    9.8s\n",
      "[Parallel(n_jobs=-1)]: Done 364 tasks      | elapsed:   11.3s\n",
      "[Parallel(n_jobs=-1)]: Done 424 tasks      | elapsed:   13.2s\n",
      "[Parallel(n_jobs=-1)]: Done 484 tasks      | elapsed:   15.1s\n"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "Best parameters found for Estimator : Bayes\n",
      "{'alpha': 4.800000000000001}\n",
      "\n",
      "Best score found for Accuracy Score metric : 0.491\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed:   15.5s finished\n"
     ],
     "name": "stderr"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AToVtWTGtIlt",
    "colab_type": "text"
   },
   "source": [
    "## Final classifier"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "-RD8o9IjtOt-",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "# Create the best classifier found from the search.\n",
    "clf = DecisionTreeClassifier(criterion='gini', max_depth=7, max_features=54, \n",
    "                             min_samples_split=2645, random_state=0)"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_oOgguTVr0ys",
    "colab_type": "text"
   },
   "source": [
    "# Apply Method"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Jd0tKq86cEea",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 935
    },
    "outputId": "91eb8c29-68ee-4f1a-9728-7f2fc8900c90"
   },
   "source": [
    "# Initialize lists to hold the accuracy results of the two methods.\n",
    "uncertainty_accuracies, random_accuracies = [], []\n",
    "\n",
    "# Initialize uncertainty sampling data.\n",
    "X_train_us = X_train.copy()\n",
    "y_train_us = y_train.copy()\n",
    "X_unlabeled_us = X_unlabeled.copy()\n",
    "y_unlabeled_us = y_hidden.copy()\n",
    "\n",
    "# Initialize random sampling data.\n",
    "X_train_rs = X_train.copy()\n",
    "y_train_rs = y_train.copy()\n",
    "X_unlabeled_rs = X_unlabeled.copy()\n",
    "y_unlabeled_rs = y_hidden.copy()\n",
    "\n",
    "# Run uncertainty sampling and random sampling methods, for 10 iterations.\n",
    "n_iterations = 10\n",
    "for i in range(n_iterations):\n",
    "    print('Iteration {}/{}'.format(i + 1, n_iterations))\n",
    "\n",
    "    print('Fitting on uncertainty sampling training set...')\n",
    "    # Train classifier with uncertainty sampling training set.\n",
    "    clf.fit(X_train_us, y_train_us)\n",
    "    # Predict on test data.\n",
    "    y_pred_us = clf.predict(X_test)\n",
    "    # Get the most uncertain sample from the unlabeled pool.\n",
    "    uncertain_sample = np.argmin(np.abs(clf.predict_proba(X_unlabeled_us) - 0.5))\n",
    "    # Calculate accuracy.\n",
    "    acc_us = accuracy_score(y_pred_us, y_test)\n",
    "    # Append current accuracy to the uncertainty accuracies array.\n",
    "    uncertainty_accuracies.append(acc_us)\n",
    "    # Update uncertainty sampling data.\n",
    "    X_train_us = np.vstack((X_train_us, X_unlabeled_us[uncertain_sample, :]))\n",
    "    y_train_us = np.hstack((y_train_us, y_unlabeled_us[uncertain_sample]))\n",
    "    X_unlabeled_us = np.delete(X_unlabeled_us, uncertain_sample, 0)\n",
    "    y_unlabeled_us = np.delete(y_unlabeled_us, uncertain_sample, 0)\n",
    "    print('Predicted accuracy was {}'.format(acc_us))\n",
    "\n",
    "    print('Fitting on random sampling training set...')\n",
    "    # Train classifier with random sampling training set.\n",
    "    clf.fit(X_train_rs, y_train_rs)\n",
    "    # Predict on test data. \n",
    "    y_pred_rs = clf.predict(X_test)\n",
    "    # Get a random unlabeled sample.\n",
    "    random_sample = randint(0, len(y_unlabeled_rs))\n",
    "    # Calculate accuracy.\n",
    "    acc_rs = accuracy_score(y_pred_rs, y_test)\n",
    "    # Append current accuracy to the random accuracies array.\n",
    "    random_accuracies.append(acc_rs)\n",
    "    # Update random sampling data.\n",
    "    X_train_rs = np.vstack((X_train_rs, X_unlabeled_rs[random_sample, :]))\n",
    "    y_train_rs = np.hstack((y_train_rs, y_unlabeled_rs[random_sample]))\n",
    "    X_unlabeled_rs = np.delete(X_unlabeled_rs, random_sample, 0)\n",
    "    y_unlabeled_rs = np.delete(y_unlabeled_rs, random_sample, 0)\n",
    "    print('Predicted accuracy was {}'.format(acc_rs))"
   ],
   "execution_count": 5,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Iteration 1/10\n",
      "Fitting on uncertainty sampling training set...\n",
      "Predicted accuracy was 0.6039156626506024\n",
      "Fitting on random sampling training set...\n",
      "Predicted accuracy was 0.6039156626506024\n",
      "Iteration 2/10\n",
      "Fitting on uncertainty sampling training set...\n",
      "Predicted accuracy was 0.6039156626506024\n",
      "Fitting on random sampling training set...\n",
      "Predicted accuracy was 0.6039156626506024\n",
      "Iteration 3/10\n",
      "Fitting on uncertainty sampling training set...\n",
      "Predicted accuracy was 0.6039156626506024\n",
      "Fitting on random sampling training set...\n",
      "Predicted accuracy was 0.6039156626506024\n",
      "Iteration 4/10\n",
      "Fitting on uncertainty sampling training set...\n",
      "Predicted accuracy was 0.6039156626506024\n",
      "Fitting on random sampling training set...\n",
      "Predicted accuracy was 0.6039156626506024\n",
      "Iteration 5/10\n",
      "Fitting on uncertainty sampling training set...\n",
      "Predicted accuracy was 0.6039156626506024\n",
      "Fitting on random sampling training set...\n",
      "Predicted accuracy was 0.6039156626506024\n",
      "Iteration 6/10\n",
      "Fitting on uncertainty sampling training set...\n",
      "Predicted accuracy was 0.6039156626506024\n",
      "Fitting on random sampling training set...\n",
      "Predicted accuracy was 0.6039156626506024\n",
      "Iteration 7/10\n",
      "Fitting on uncertainty sampling training set...\n",
      "Predicted accuracy was 0.6039156626506024\n",
      "Fitting on random sampling training set...\n",
      "Predicted accuracy was 0.6039156626506024\n",
      "Iteration 8/10\n",
      "Fitting on uncertainty sampling training set...\n",
      "Predicted accuracy was 0.6039156626506024\n",
      "Fitting on random sampling training set...\n",
      "Predicted accuracy was 0.6039156626506024\n",
      "Iteration 9/10\n",
      "Fitting on uncertainty sampling training set...\n",
      "Predicted accuracy was 0.6039156626506024\n",
      "Fitting on random sampling training set...\n",
      "Predicted accuracy was 0.6039156626506024\n",
      "Iteration 10/10\n",
      "Fitting on uncertainty sampling training set...\n",
      "Predicted accuracy was 0.6039156626506024\n",
      "Fitting on random sampling training set...\n",
      "Predicted accuracy was 0.6039156626506024\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3ziyppe0oFUT",
    "colab_type": "text"
   },
   "source": [
    "## Plot Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P6hQIphSyBAi",
    "colab_type": "text"
   },
   "source": [
    "First, we visualize the hyperplanes an SVM creates, using the two different methods. \n",
    "We use t-SNE, in order to reduce the dimensionality of the data."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "YoaX_aDBzN6U",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 623
    },
    "outputId": "39b4ea0d-d648-43de-e014-fb183279f415"
   },
   "source": [
    "def plot_hyperplane(X, y, classifier) -> None:\n",
    "    \"\"\"\n",
    "    Plots separating hyperplane.\n",
    "\n",
    "    :param X: the data.\n",
    "    :param y: the labels.\n",
    "    :param classifier: the classifier to be used.\n",
    "    \"\"\"\n",
    "    # Scatter data.\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, s=30, cmap=plt.cm.Paired)\n",
    "\n",
    "    # plot the decision function\n",
    "    ax = plt.gca()\n",
    "    xlim = ax.get_xlim()\n",
    "    ylim = ax.get_ylim()\n",
    "\n",
    "    # create grid to evaluate model\n",
    "    xx = np.linspace(xlim[0], xlim[1], 30)\n",
    "    yy = np.linspace(ylim[0], ylim[1], 30)\n",
    "    YY, XX = np.meshgrid(yy, xx)\n",
    "    xy = np.vstack([XX.ravel(), YY.ravel()]).T\n",
    "    Z = classifier.decision_function(xy).reshape(XX.shape)\n",
    "\n",
    "    # plot decision boundary and margins\n",
    "    ax.contour(XX, YY, Z, colors='k', levels=[-1, 0, 1], alpha=0.5,\n",
    "               linestyles=['--', '-', '--'])\n",
    "    # plot support vectors\n",
    "    ax.scatter(classifier.support_vectors_[:, 0], classifier.support_vectors_[:, 1], s=100,\n",
    "               linewidth=1, facecolors='none', edgecolors='k')\n",
    "    plt.show()\n",
    "    \n",
    "# Reduce data dimensionality, using t-SNE.\n",
    "print('Reducing dimensionality for uncertainty sampling result data.')\n",
    "X_embedded_us = TSNE(verbose=1).fit_transform(X_train_us)\n",
    "print('Reducing dimensionality for random sampling result data.')\n",
    "X_embedded_rs = TSNE(verbose=1).fit_transform(X_train_rs)\n",
    "\n",
    "# Plot hyperplanes.\n",
    "clf_us, clf_rs = SVC(C=110, kernel='linear', random_state=0), \\\n",
    "                 SVC(C=110, kernel='linear', random_state=0)\n",
    "print('Fitting an SVM to the embedded uncertainty sampling result data...')\n",
    "clf_us.fit(X_embedded_us, y_train_us)\n",
    "print('Fitting an SVM to the embedded random sampling result data...')\n",
    "clf_rs.fit(X_embedded_rs, y_train_rs)\n",
    "\n",
    "plot_hyperplane(X_embedded_us, y_train_us, clf_us)\n",
    "plot_hyperplane(X_embedded_rs, y_train_rs, clf_rs)"
   ],
   "execution_count": 0,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Reducing dimensionality for uncertainty sampling result data.\n",
      "[t-SNE] Computing 91 nearest neighbors...\n",
      "[t-SNE] Indexed 8261 samples in 0.118s...\n",
      "[t-SNE] Computed neighbors for 8261 samples in 34.325s...\n",
      "[t-SNE] Computed conditional probabilities for sample 1000 / 8261\n",
      "[t-SNE] Computed conditional probabilities for sample 2000 / 8261\n",
      "[t-SNE] Computed conditional probabilities for sample 3000 / 8261\n",
      "[t-SNE] Computed conditional probabilities for sample 4000 / 8261\n",
      "[t-SNE] Computed conditional probabilities for sample 5000 / 8261\n",
      "[t-SNE] Computed conditional probabilities for sample 6000 / 8261\n",
      "[t-SNE] Computed conditional probabilities for sample 7000 / 8261\n",
      "[t-SNE] Computed conditional probabilities for sample 8000 / 8261\n",
      "[t-SNE] Computed conditional probabilities for sample 8261 / 8261\n",
      "[t-SNE] Mean sigma: 5203.689845\n",
      "[t-SNE] KL divergence after 250 iterations with early exaggeration: 95.067833\n",
      "[t-SNE] KL divergence after 1000 iterations: 3.011194\n",
      "Reducing dimensionality for random sampling result data.\n",
      "[t-SNE] Computing 91 nearest neighbors...\n",
      "[t-SNE] Indexed 8261 samples in 0.113s...\n",
      "[t-SNE] Computed neighbors for 8261 samples in 34.746s...\n",
      "[t-SNE] Computed conditional probabilities for sample 1000 / 8261\n",
      "[t-SNE] Computed conditional probabilities for sample 2000 / 8261\n",
      "[t-SNE] Computed conditional probabilities for sample 3000 / 8261\n",
      "[t-SNE] Computed conditional probabilities for sample 4000 / 8261\n",
      "[t-SNE] Computed conditional probabilities for sample 5000 / 8261\n",
      "[t-SNE] Computed conditional probabilities for sample 6000 / 8261\n",
      "[t-SNE] Computed conditional probabilities for sample 7000 / 8261\n",
      "[t-SNE] Computed conditional probabilities for sample 8000 / 8261\n",
      "[t-SNE] Computed conditional probabilities for sample 8261 / 8261\n",
      "[t-SNE] Mean sigma: 5208.020929\n",
      "[t-SNE] KL divergence after 250 iterations with early exaggeration: 90.712471\n",
      "[t-SNE] KL divergence after 1000 iterations: 3.008831\n",
      "Fitting an SVM to the embedded uncertainty sampling result data...\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8MKN5VGN1InO",
    "colab_type": "text"
   },
   "source": [
    "We notice that the "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LMZRnS2kqcFN",
    "colab_type": "text"
   },
   "source": [
    "Finally, we plot the learning curves of the two methods, in order to compare them."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "mz92hVF4oCC2",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "# Create a figure.\n",
    "plt.figure()\n",
    "# Set title and labels.\n",
    "plt.title('Uncertainty Sampling vs Random Sampling Learning Curves')\n",
    "plt.xlabel('Number Of Instance Queries')\n",
    "plt.ylabel('Accuracy')\n",
    "\n",
    "# Create a plot grid.\n",
    "plt.grid()\n",
    "# Plot uncertainty and random sampling learning curves.\n",
    "plt.plot(range(n_iterations), uncertainty_accuracies, 'o-', color=\"r\",\n",
    "         label=\"Uncertainty Sampling\")\n",
    "plt.plot(range(n_iterations), random_accuracies, 'o-', color=\"g\",\n",
    "         label=\"Random Sampling\")\n",
    "\n",
    "# Show legends, placed at the best possible location.\n",
    "plt.legend(loc=\"best\")\n",
    "# Show plot.\n",
    "plt.show()"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rxLTJtZGqqBf",
    "colab_type": "text"
   },
   "source": [
    "As we can see, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "klzbZ6KWDoax",
    "colab_type": "text"
   },
   "source": [
    "# Conclusion"
   ]
  }
 ]
}